{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in ./t5-streamlit/lib/python3.9/site-packages (4.34.0)\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in ./t5-streamlit/lib/python3.9/site-packages (0.14.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in ./t5-streamlit/lib/python3.9/site-packages (0.1.99)\n",
      "Requirement already satisfied: huggingface_hub in ./t5-streamlit/lib/python3.9/site-packages (0.17.3)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in ./t5-streamlit/lib/python3.9/site-packages (from transformers[torch]) (2.1.0)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in ./t5-streamlit/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in ./t5-streamlit/lib/python3.9/site-packages (from datasets) (2.1.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./t5-streamlit/lib/python3.9/site-packages (from datasets) (2023.9.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in ./t5-streamlit/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./t5-streamlit/lib/python3.9/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: psutil in ./t5-streamlit/lib/python3.9/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./t5-streamlit/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./t5-streamlit/lib/python3.9/site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./t5-streamlit/lib/python3.9/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./t5-streamlit/lib/python3.9/site-packages (from requests->transformers[torch]) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./t5-streamlit/lib/python3.9/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: sympy in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./t5-streamlit/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./t5-streamlit/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.10->transformers[torch]) (12.2.140)\n",
      "Requirement already satisfied: click in ./t5-streamlit/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./t5-streamlit/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./t5-streamlit/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./t5-streamlit/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./t5-streamlit/lib/python3.9/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./t5-streamlit/lib/python3.9/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=249a4aa3ed26fb0ce678344fb27c55d68f67e29ed3369cd457d8166b26726ccb\n",
      "  Stored in directory: /home/yaochung41/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: xxhash, multidict, joblib, frozenlist, dill, async-timeout, absl-py, yarl, responses, nltk, multiprocess, aiosignal, rouge_score, aiohttp, transformers, accelerate, datasets, evaluate\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0\n",
      "    Uninstalling transformers-4.34.0:\n",
      "      Successfully uninstalled transformers-4.34.0\n",
      "Successfully installed absl-py-2.0.0 accelerate-0.24.0 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 frozenlist-1.4.0 joblib-1.3.2 multidict-6.0.4 multiprocess-0.70.15 nltk-3.8.1 responses-0.18.0 rouge_score-0.1.2 transformers-4.34.1 xxhash-3.4.1 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch] tokenizers datasets evaluate rouge_score sentencepiece huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaochung41/github/t5-streamlit/t5-streamlit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Create a dictionary containing the questions and answers\n",
    "data_dict = {\n",
    "    \"question\": [\n",
    "        \"What is your name?\",\n",
    "        \"How old are you?\",\n",
    "        \"Where are you from?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"My name is John.\",\n",
    "        \"I am 30 years old.\",\n",
    "        \"I am from New York.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a dataset from the dictionary\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset = dataset.select([0, 1])\n",
    "test_dataset = dataset.select([2])\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 246kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 892M/892M [01:17<00:00, 11.5MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 24.7kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.85MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 9.35MB/s]\n",
      "/home/yaochung41/github/t5-streamlit/t5-streamlit/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base = \"t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(base)\n",
    "tokenizer = T5Tokenizer.from_pretrained(base)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 477.90 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 258.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# prefix our task\n",
    "prefix1 = \"rationale + \"\n",
    "prefix2 = \"label + \"\n",
    "\n",
    "# define preprocessing function\n",
    "def preprocess_function(examples):\n",
    "  \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "  # the \"inputs\" are the tokenized answer\n",
    "  inputs = [prefix1 + doc for doc in examples[\"question\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "  # the \"labels\" are the tokenized outputs\n",
    "  labels = [prefix2 + doc for doc in examples[\"answer\"]]\n",
    "  labels_token = tokenizer(labels, max_length=512, truncation=True)\n",
    "  model_inputs[\"labels\"] = labels_token[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "# Map the preprocessing function across our datasets\n",
    "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 3.86MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Use Rougue score to evaluate the training process\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  preds, labels = eval_preds\n",
    "\n",
    "  #decode preds and labels\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  #rougeLSum expects newline after each sentence\n",
    "  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "  result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training arguments: for GPU with 16GB VRAM\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.84s/it]/home/yaochung41/github/t5-streamlit/t5-streamlit/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                             \n",
      " 50%|█████     | 1/2 [00:02<00:01,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3975639343261719, 'eval_rouge1': 0.1818181818181818, 'eval_rouge2': 0.0, 'eval_rougeL': 0.1818181818181818, 'eval_rougeLsum': 0.1818181818181818, 'eval_runtime': 0.5976, 'eval_samples_per_second': 1.673, 'eval_steps_per_second': 1.673, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0151854753494263, 'eval_rouge1': 0.1818181818181818, 'eval_rouge2': 0.0, 'eval_rougeL': 0.1818181818181818, 'eval_rougeLsum': 0.1818181818181818, 'eval_runtime': 0.5788, 'eval_samples_per_second': 1.728, 'eval_steps_per_second': 1.728, 'epoch': 2.0}\n",
      "{'train_runtime': 4.6405, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.431, 'train_loss': 1.6592155694961548, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=1.6592155694961548, metrics={'train_runtime': 4.6405, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.431, 'train_loss': 1.6592155694961548, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5-streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
